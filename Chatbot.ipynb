{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1FaBY5mvIuO",
        "outputId": "df704df0-9549-4840-edf2-48bc3557f114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (8.4.0)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127283 sha256=1a9b58fa5f78c93615e52efb1d594709ca2d2d86617e526aef5e80ad934e4e25\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/fb/7b/e06204a0ceefa45443930b9a250cb5ebe31def0e4e8245a465\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0xPc2h1tPBL",
        "outputId": "2f550ec3-4269-4901-8a66-5af7f50c1c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import json\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import random\n",
        "import tflearn\n",
        "import numpy as np\n",
        "import pickle\n",
        "stemmer = LancasterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZgo7csoibni",
        "outputId": "7a93b420-cb4c-43c9-edc9-bfbc16ea458b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'tensorflow.compat.v1.version' from '/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/v1/version/__init__.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phlk6pMUvPIu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "2b3e1beb-4fbf-4cb1-87ac-49f09a7068a0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c949ec1c5fe2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tokenize words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Intents.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mphile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Intents.json'"
          ]
        }
      ],
      "source": [
        "#tokenize words\n",
        "with open('Intents.json') as file:\n",
        "  data=json.load(file)\n",
        "try:\n",
        "  with open(\"data.pickle\",'rb') as phile:\n",
        "      words,labels,Training,Testing=pickle.load(phile)\n",
        "except:\n",
        "  words=[]\n",
        "  labels=[]\n",
        "  doc_x=[]\n",
        "  doc_y=[]\n",
        "  for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "       wrds= nltk.word_tokenize(pattern)\n",
        "       words.extend(wrds)\n",
        "       doc_x.append(wrds)\n",
        "       doc_y.append(intent['tag'])\n",
        "    if intent['tag'] not in labels:\n",
        "       labels.append(intent['tag'])\n",
        "\n",
        "  words = [stemmer.stem(w.lower()) for w in words if w != '?' or w != '!']\n",
        "  words= sorted(list(set(words)))\n",
        "  labels=sorted(labels)\n",
        "  #bag of words\n",
        "  Training=[]\n",
        "  Testing=[]\n",
        "\n",
        "  output=[0 for _ in range(len(labels))]\n",
        "\n",
        "  for x, doc in enumerate(doc_x):\n",
        "     bag=[]\n",
        "     wrds=[stemmer.stem(w.lower()) for w in doc]\n",
        "     for w in words:\n",
        "       if w in wrds:\n",
        "         bag.append(1)\n",
        "       else:\n",
        "         bag.append(0)\n",
        "\n",
        "     Testing_row=output[:]\n",
        "     Testing_row[labels.index(doc_y[x])] = 1\n",
        "\n",
        "     Training.append(bag)\n",
        "     Testing.append(Testing_row)\n",
        "\n",
        "  Training=np.array(Training)\n",
        "  Testing=np.array(Testing)\n",
        "  with open(\"data.pickle\",\"wb\") as phile:\n",
        "    pickle.dump((words,labels,Training,Testing),phile)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5u_nUPjW6NR",
        "outputId": "a4dfcb42-3fdb-4ad0-d062-5bf2ca7539bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBbTZAoV0XO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11eb1d9-3d4a-4542-c30e-9f1b7527b167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ],
      "source": [
        "tf.reset_default_graph()\n",
        "network=tflearn.input_data(shape=[None,len(Training[0])])\n",
        "network=tflearn.fully_connected(network,8)\n",
        "network=tflearn.fully_connected(network,8)\n",
        "network=tflearn.fully_connected(network,len(Testing[0]),activation='softmax')\n",
        "network= tflearn.regression(network)\n",
        "model=tflearn.DNN(network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH51J4eS2AkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c6ee46-59df-420c-fee0-7d50e8a03b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.29187\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 500 | loss: 0.29187 - acc: 0.9718 -- iter: 24/25\n",
            "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.31268\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 500 | loss: 0.31268 - acc: 0.9746 -- iter: 25/25\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "model.fit(Training, Testing, n_epoch=500, batch_size=8, show_metric=True)\n",
        "model.save(\"model.tflearn\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bag(inp,words):\n",
        "    s = [0 for _ in range(len(words))]\n",
        "    t = nltk.word_tokenize(inp)\n",
        "    t = [stemmer.stem(w.lower()) for w in t]\n",
        "    for se in t:\n",
        "        for i,x in enumerate(words):\n",
        "           if x == se:\n",
        "              s[i]=1\n",
        "    return np.array(s)\n",
        "bot_name='AVA'"
      ],
      "metadata": {
        "id": "DnfztcvDvo0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    while True:\n",
        "        inp = input('Warlock: ')\n",
        "        if inp.lower() == 'quit' or inp.lower() == 'signing off':\n",
        "           break\n",
        "        result = model.predict([bag(inp,words)])\n",
        "        index = np.argmax(result)\n",
        "        tag = labels[index]\n",
        "        for tg in data['intents']:\n",
        "            if tg['tag'] == tag:\n",
        "              responses = tg['responses']\n",
        "\n",
        "        print(random.choice(responses))\n",
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doJ4GV90xgX6",
        "outputId": "3e55fc0a-e1b8-4761-d060-f1a5c6992073"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warlock: hey AVA\n",
            "I am Augumented Virtual Assistant\n",
            "Warlock: Hey AVA\n",
            "I am Augumented Virtual Assistant\n",
            "Warlock: what's up\n",
            "I'm still learning. Can you provide more information?\n",
            "Warlock: how are you \n",
            "Hi there!\n",
            "Warlock: signing off\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}